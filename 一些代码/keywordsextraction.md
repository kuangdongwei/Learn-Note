{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache C:\\Users\\ADMINI~1\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 1.262 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF模型结果：\n",
      "城市/ 社会/ 爱心/ 晋江市/ 基金会/ 中国/ 中华/ 大会/ 公益活动/ 陈健倩/ \n",
      "TextRank模型结果：\n",
      "城市/ 爱心/ 救助/ 中国/ 社会/ 晋江市/ 基金会/ 大会/ 介绍/ 公益活动/ \n",
      "LSI模型结果：\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "failed to create intent(cache|hide)|optional array-- must have defined dimensions but got (0,)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-0e9f0350e53f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    272\u001b[0m     \u001b[0mtextrank_extract\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    273\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'LSI模型结果：'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 274\u001b[1;33m     \u001b[0mtopic_extract\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilter_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'LSI'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    275\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'LDA模型结果：'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    276\u001b[0m     \u001b[0mtopic_extract\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilter_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'LDA'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-0e9f0350e53f>\u001b[0m in \u001b[0;36mtopic_extract\u001b[1;34m(word_list, model, pos, keyword_num)\u001b[0m\n\u001b[0;32m    246\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mtopic_extract\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeyword_num\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    247\u001b[0m     \u001b[0mdoc_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpos\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 248\u001b[1;33m     \u001b[0mtopic_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTopicModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeyword_num\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    249\u001b[0m     \u001b[0mtopic_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_simword\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    250\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-0e9f0350e53f>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, doc_list, keyword_num, model, num_topics)\u001b[0m\n\u001b[0;32m    158\u001b[0m         \u001b[1;31m# 选择加载的模型\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'LSI'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 160\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_lsi\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    161\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    162\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_lda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-0e9f0350e53f>\u001b[0m in \u001b[0;36mtrain_lsi\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    167\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtrain_lsi\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 169\u001b[1;33m         \u001b[0mlsi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLsiModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus_tfidf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mid2word\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_topics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_topics\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    170\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlsi\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Myanaconda\\anaconda\\lib\\site-packages\\gensim\\models\\lsimodel.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, corpus, num_topics, id2word, chunksize, decay, distributed, onepass, power_iters, extra_samples, dtype)\u001b[0m\n\u001b[0;32m    443\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    444\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcorpus\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 445\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_documents\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    446\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    447\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0madd_documents\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecay\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Myanaconda\\anaconda\\lib\\site-packages\\gensim\\models\\lsimodel.py\u001b[0m in \u001b[0;36madd_documents\u001b[1;34m(self, corpus, chunksize, decay)\u001b[0m\n\u001b[0;32m    510\u001b[0m                         update = Projection(\n\u001b[0;32m    511\u001b[0m                             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_terms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_topics\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjob\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mextra_dims\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextra_samples\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 512\u001b[1;33m                             \u001b[0mpower_iters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpower_iters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    513\u001b[0m                         )\n\u001b[0;32m    514\u001b[0m                         \u001b[1;32mdel\u001b[0m \u001b[0mjob\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Myanaconda\\anaconda\\lib\\site-packages\\gensim\\models\\lsimodel.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, m, k, docs, use_svdlibc, power_iters, extra_dims, dtype)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mdocs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmaxsize\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m                     \u001b[0mnum_terms\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpower_iters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpower_iters\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                     extra_dims=self.extra_dims, dtype=dtype)\n\u001b[0m\u001b[0;32m    200\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Myanaconda\\anaconda\\lib\\site-packages\\gensim\\models\\lsimodel.py\u001b[0m in \u001b[0;36mstochastic_svd\u001b[1;34m(corpus, rank, num_terms, chunksize, extra_dims, power_iters, dtype, eps)\u001b[0m\n\u001b[0;32m    935\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"orthonormalizing %s action matrix\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    936\u001b[0m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 937\u001b[1;33m         \u001b[0mq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmatutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mqr_destroy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# orthonormalize the range\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    938\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    939\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"running %i power iterations\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpower_iters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Myanaconda\\anaconda\\lib\\site-packages\\gensim\\matutils.py\u001b[0m in \u001b[0;36mqr_destroy\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m   1176\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"computing QR of %s dense matrix\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1177\u001b[0m     \u001b[0mgeqrf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_lapack_funcs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'geqrf'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1178\u001b[1;33m     \u001b[0mqr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtau\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwork\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgeqrf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlwork\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moverwrite_a\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1179\u001b[0m     \u001b[0mqr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtau\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwork\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgeqrf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlwork\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mwork\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moverwrite_a\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1180\u001b[0m     \u001b[1;32mdel\u001b[0m \u001b[0ma\u001b[0m  \u001b[1;31m# free up mem\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: failed to create intent(cache|hide)|optional array-- must have defined dimensions but got (0,)"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "import jieba\n",
    "import jieba.posseg as psg\n",
    "from gensim import corpora, models\n",
    "from jieba import analyse\n",
    "import functools\n",
    "\n",
    "\n",
    "# 停用词表加载方法\n",
    "def get_stopword_list():\n",
    "    # 停用词表存储路径，每一行为一个词，按行读取进行加载\n",
    "    # 进行编码转换确保匹配准确率\n",
    "    stop_word_path = 'datas/stopword.txt'\n",
    "    with open(stop_word_path,encoding='UTF-8') as f:\n",
    "        stopwordtxt = f.read()\n",
    "    stopword_list = [sw.replace('\\n', '') for sw in stopwordtxt]\n",
    "    return stopword_list\n",
    "\n",
    "\n",
    "# 分词方法，调用结巴接口\n",
    "def seg_to_list(sentence, pos=False):\n",
    "    if not pos:\n",
    "        # 不进行词性标注的分词方法\n",
    "        seg_list = jieba.cut(sentence)\n",
    "    else:\n",
    "        # 进行词性标注的分词方法\n",
    "        seg_list = psg.cut(sentence)\n",
    "    return seg_list\n",
    "\n",
    "\n",
    "# 去除干扰词\n",
    "def word_filter(seg_list, pos=False):\n",
    "    stopword_list = get_stopword_list()\n",
    "    filter_list = []\n",
    "    # 根据POS参数选择是否词性过滤\n",
    "    ## 不进行词性过滤，则将词性都标记为n，表示全部保留\n",
    "    for seg in seg_list:\n",
    "        if not pos:\n",
    "            word = seg\n",
    "            flag = 'n'\n",
    "        else:\n",
    "            word = seg.word\n",
    "            flag = seg.flag\n",
    "        if not flag.startswith('n'):\n",
    "            continue\n",
    "        # 过滤停用词表中的词，以及长度为<2的词\n",
    "        if not word in stopword_list and len(word) > 1:\n",
    "            filter_list.append(word)\n",
    "\n",
    "    return filter_list\n",
    "\n",
    "\n",
    "# 数据加载，pos为是否词性标注的参数，corpus_path为数据集路径\n",
    "def load_data(pos=False, corpus_path='datas/corpus.txt'):\n",
    "    # 调用上面方式对数据集进行处理，处理后的每条数据仅保留非干扰词\n",
    "    doc_list = []\n",
    "    with open(corpus_path,encoding='UTF-8') as f:\n",
    "        coupustxt = f.read()\n",
    "    for line in coupustxt:\n",
    "        content = line.strip()\n",
    "        seg_list = seg_to_list(content, pos)\n",
    "        filter_list = word_filter(seg_list, pos)\n",
    "        doc_list.append(filter_list)\n",
    "\n",
    "    return doc_list\n",
    "\n",
    "\n",
    "# idf值统计方法\n",
    "def train_idf(doc_list):\n",
    "    idf_dic = {}\n",
    "    # 总文档数\n",
    "    tt_count = len(doc_list)\n",
    "\n",
    "    # 每个词出现的文档数\n",
    "    for doc in doc_list:\n",
    "        for word in set(doc):\n",
    "            idf_dic[word] = idf_dic.get(word, 0.0) + 1.0\n",
    "\n",
    "    # 按公式转换为idf值，分母加1进行平滑处理\n",
    "    for k, v in idf_dic.items():\n",
    "        idf_dic[k] = math.log(tt_count / (1.0 + v))\n",
    "\n",
    "    # 对于没有在字典中的词，默认其仅在一个文档出现，得到默认idf值\n",
    "    default_idf = math.log(tt_count / (1.0))\n",
    "    return idf_dic, default_idf\n",
    "\n",
    "\n",
    "#  排序函数，用于topK关键词的按值排序\n",
    "def cmp(e1, e2):\n",
    "    import numpy as np\n",
    "    res = np.sign(e1[1] - e2[1])\n",
    "    if res != 0:\n",
    "        return res\n",
    "    else:\n",
    "        a = e1[0] + e2[0]\n",
    "        b = e2[0] + e1[0]\n",
    "        if a > b:\n",
    "            return 1\n",
    "        elif a == b:\n",
    "            return 0\n",
    "        else:\n",
    "            return -1\n",
    "\n",
    "# TF-IDF类\n",
    "class TfIdf(object):\n",
    "    # 四个参数分别是：训练好的idf字典，默认idf值，处理后的待提取文本，关键词数量\n",
    "    def __init__(self, idf_dic, default_idf, word_list, keyword_num):\n",
    "        self.word_list = word_list\n",
    "        self.idf_dic, self.default_idf = idf_dic, default_idf\n",
    "        self.tf_dic = self.get_tf_dic()\n",
    "        self.keyword_num = keyword_num\n",
    "\n",
    "    # 统计tf值\n",
    "    def get_tf_dic(self):\n",
    "        tf_dic = {}\n",
    "        for word in self.word_list:\n",
    "            tf_dic[word] = tf_dic.get(word, 0.0) + 1.0\n",
    "\n",
    "        tt_count = len(self.word_list)\n",
    "        for k, v in tf_dic.items():\n",
    "            tf_dic[k] = float(v) / tt_count\n",
    "\n",
    "        return tf_dic\n",
    "\n",
    "    # 按公式计算tf-idf\n",
    "    def get_tfidf(self):\n",
    "        tfidf_dic = {}\n",
    "        for word in self.word_list:\n",
    "            idf = self.idf_dic.get(word, self.default_idf)\n",
    "            tf = self.tf_dic.get(word, 0)\n",
    "\n",
    "            tfidf = tf * idf\n",
    "            tfidf_dic[word] = tfidf\n",
    "\n",
    "        tfidf_dic.items()\n",
    "        # 根据tf-idf排序，去排名前keyword_num的词作为关键词\n",
    "        for k, v in sorted(tfidf_dic.items(), key=functools.cmp_to_key(cmp), reverse=True)[:self.keyword_num]:\n",
    "            print(k + \"/ \", end='')\n",
    "        print()\n",
    "\n",
    "\n",
    "# 主题模型\n",
    "class TopicModel(object):\n",
    "    # 三个传入参数：处理后的数据集，关键词数量，具体模型（LSI、LDA），主题数量\n",
    "    def __init__(self, doc_list, keyword_num, model='LSI', num_topics=4):\n",
    "        # 使用gensim的接口，将文本转为向量化表示\n",
    "        # 先构建词空间\n",
    "        self.dictionary = corpora.Dictionary(doc_list)\n",
    "        # 使用BOW模型向量化\n",
    "        corpus = [self.dictionary.doc2bow(doc) for doc in doc_list]\n",
    "        # 对每个词，根据tf-idf进行加权，得到加权后的向量表示\n",
    "        self.tfidf_model = models.TfidfModel(corpus)\n",
    "        self.corpus_tfidf = self.tfidf_model[corpus]\n",
    "\n",
    "        self.keyword_num = keyword_num\n",
    "        self.num_topics = num_topics\n",
    "        # 选择加载的模型\n",
    "        if model == 'LSI':\n",
    "            self.model = self.train_lsi()\n",
    "        else:\n",
    "            self.model = self.train_lda()\n",
    "\n",
    "        # 得到数据集的主题-词分布\n",
    "        word_dic = self.word_dictionary(doc_list)\n",
    "        self.wordtopic_dic = self.get_wordtopic(word_dic)\n",
    "\n",
    "    def train_lsi(self):\n",
    "        lsi = models.LsiModel(self.corpus_tfidf, id2word=self.dictionary, num_topics=self.num_topics)\n",
    "        return lsi\n",
    "\n",
    "    def train_lda(self):\n",
    "        lda = models.LdaModel(self.corpus_tfidf, id2word=self.dictionary, num_topics=self.num_topics)\n",
    "        return lda\n",
    "\n",
    "    def get_wordtopic(self, word_dic):\n",
    "        wordtopic_dic = {}\n",
    "\n",
    "        for word in word_dic:\n",
    "            single_list = [word]\n",
    "            wordcorpus = self.tfidf_model[self.dictionary.doc2bow(single_list)]\n",
    "            wordtopic = self.model[wordcorpus]\n",
    "            wordtopic_dic[word] = wordtopic\n",
    "        return wordtopic_dic\n",
    "\n",
    "    # 计算词的分布和文档的分布的相似度，取相似度最高的keyword_num个词作为关键词\n",
    "    def get_simword(self, word_list):\n",
    "        sentcorpus = self.tfidf_model[self.dictionary.doc2bow(word_list)]\n",
    "        senttopic = self.model[sentcorpus]\n",
    "\n",
    "        # 余弦相似度计算\n",
    "        def calsim(l1, l2):\n",
    "            a, b, c = 0.0, 0.0, 0.0\n",
    "            for t1, t2 in zip(l1, l2):\n",
    "                x1 = t1[1]\n",
    "                x2 = t2[1]\n",
    "                a += x1 * x1\n",
    "                b += x1 * x1\n",
    "                c += x2 * x2\n",
    "            sim = a / math.sqrt(b * c) if not (b * c) == 0.0 else 0.0\n",
    "            return sim\n",
    "\n",
    "        # 计算输入文本和每个词的主题分布相似度\n",
    "        sim_dic = {}\n",
    "        for k, v in self.wordtopic_dic.items():\n",
    "            if k not in word_list:\n",
    "                continue\n",
    "            sim = calsim(v, senttopic)\n",
    "            sim_dic[k] = sim\n",
    "\n",
    "        for k, v in sorted(sim_dic.items(), key=functools.cmp_to_key(cmp), reverse=True)[:self.keyword_num]:\n",
    "            print(k + \"/ \", end='')\n",
    "        print()\n",
    "\n",
    "    # 词空间构建方法和向量化方法，在没有gensim接口时的一般处理方法\n",
    "    def word_dictionary(self, doc_list):\n",
    "        dictionary = []\n",
    "        for doc in doc_list:\n",
    "            dictionary.extend(doc)\n",
    "\n",
    "        dictionary = list(set(dictionary))\n",
    "\n",
    "        return dictionary\n",
    "\n",
    "    def doc2bowvec(self, word_list):\n",
    "        vec_list = [1 if word in word_list else 0 for word in self.dictionary]\n",
    "        return vec_list\n",
    "\n",
    "\n",
    "def tfidf_extract(word_list, pos=False, keyword_num=10):\n",
    "    doc_list = load_data(pos)\n",
    "    idf_dic, default_idf = train_idf(doc_list)\n",
    "    tfidf_model = TfIdf(idf_dic, default_idf, word_list, keyword_num)\n",
    "    tfidf_model.get_tfidf()\n",
    "\n",
    "\n",
    "def textrank_extract(text, pos=False, keyword_num=10):\n",
    "    textrank = analyse.textrank\n",
    "    keywords = textrank(text, keyword_num)\n",
    "    # 输出抽取出的关键词\n",
    "    for keyword in keywords:\n",
    "        print(keyword + \"/ \", end='')\n",
    "    print()\n",
    "\n",
    "\n",
    "def topic_extract(word_list, model, pos=False, keyword_num=10):\n",
    "    doc_list = load_data(pos)\n",
    "    topic_model = TopicModel(doc_list, keyword_num, model=model)\n",
    "    topic_model.get_simword(word_list)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    text = '6月19日,《2012年度“中国爱心城市”公益活动新闻发布会》在京举行。' + \\\n",
    "           '中华社会救助基金会理事长许嘉璐到会讲话。基金会高级顾问朱发忠,全国老龄' + \\\n",
    "           '办副主任朱勇,民政部社会救助司助理巡视员周萍,中华社会救助基金会副理事长耿志远,' + \\\n",
    "           '重庆市民政局巡视员谭明政。晋江市人大常委会主任陈健倩,以及10余个省、市、自治区民政局' + \\\n",
    "           '领导及四十多家媒体参加了发布会。中华社会救助基金会秘书长时正新介绍本年度“中国爱心城' + \\\n",
    "           '市”公益活动将以“爱心城市宣传、孤老关爱救助项目及第二届中国爱心城市大会”为主要内容,重庆市' + \\\n",
    "           '、呼和浩特市、长沙市、太原市、蚌埠市、南昌市、汕头市、沧州市、晋江市及遵化市将会积极参加' + \\\n",
    "           '这一公益活动。中国雅虎副总编张银生和凤凰网城市频道总监赵耀分别以各自媒体优势介绍了活动' + \\\n",
    "           '的宣传方案。会上,中华社会救助基金会与“第二届中国爱心城市大会”承办方晋江市签约,许嘉璐理' + \\\n",
    "           '事长接受晋江市参与“百万孤老关爱行动”向国家重点扶贫地区捐赠的价值400万元的款物。晋江市人大' + \\\n",
    "           '常委会主任陈健倩介绍了大会的筹备情况。'\n",
    "\n",
    "    pos = True\n",
    "    seg_list = seg_to_list(text, pos)\n",
    "    filter_list = word_filter(seg_list, pos)\n",
    "\n",
    "    print('TF-IDF模型结果：')\n",
    "    tfidf_extract(filter_list)\n",
    "    print('TextRank模型结果：')\n",
    "    textrank_extract(text)\n",
    "    print('LSI模型结果：')\n",
    "    topic_extract(filter_list, 'LSI', pos)\n",
    "    print('LDA模型结果：')\n",
    "    topic_extract(filter_list, 'LDA', pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hekko\n"
     ]
    }
   ],
   "source": [
    "print('hekko')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
